---
title: "COMPSCIX 415.2 Homework 7"
author: "Bryan Hee"
date: "March 19, 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE, echo =FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
```

###Exercise 1
There are 81 columns or variables and 1,460 observations in the train data set from the kaggle competition, House Prices: Advanced Regression Techniques.

```{r import data, results=FALSE, warning=FALSE, message=FALSE}
train <- read_csv(file = "C:/Users/BryanHee/OneDrive - stok LLC/Intro to Data Science/HW Assignments/Assignment 7/train.csv")
glimpse(train)
```

###Exercise 2
The following code represents a random sample of the train dataset via a 70/30 split. 70% of the train dataset will be used as a training dataset. The remaining 30% will be used as the testing dataset.

```{r train/test 70/30 dataset creation}
set.seed(29283)

train_set <- train %>% sample_frac(0.7)
test_set <- train %>% filter(!(train$Id %in% train_set$Id))
```

###Exercise 3
The following code provides a linear regression with only a y-intercept for the variable train_set$SalePrice. The mean Sale Price is $181,176. This is confirmed by the function broom::tidy. The R-squared value is derived from the glance() function which is 0.

```{r simple linear regression model}
mod_0 <- lm(formula = SalePrice ~ 1, data = train_set)

mean(train_set$SalePrice)
tidy(mod_0)
glance(mod_0)
```

###Exercise 4
It makes sense that the above ground living square footage of the building (GrLivArea) is positively correlated to the house price. The coefficient provides the price per square foot valuation of the linear model, $54.50/sf. The overall quality of the building material (OverallQual) also makes sense that it is positievely correlated. The higher the 1-10 rating, the more expensive the house (all else being equal). The coefficient is the price effect on the overall home sale price associated with increasing the material and finish rating by one. 

I would interpret the Neighborhood coefficients in the following way: the sign describes whether or not the neighborhood is beneficial for the home price (i.e. whether it is a desirable neighborhood (+) or not (-), and the absolute value relates the weight factor of the desirability (i.e. the larger the positive number the more desirable the neighborhood).

Using a cutoff p-value of 0.5, all of the features are significant. However, in my opinion Neighborhood feature is not a practically significant feature due to the small sample size of the train_set dataset. There are 6 neighborhoods with less than 15 home sales within the sample set. That is not enough to have this represent a meaningful feature. See the code below for the count per each neighborhood.

The model is a relatively good fit for the training set given the high R-squared value of 0.787.

```{r 3-feature linear regression model}
mod_1 <- lm(SalePrice ~ GrLivArea + OverallQual + Neighborhood, data = train_set)
tidy(mod_1)

train_set %>%
  group_by(Neighborhood) %>%
summarise(count = n()) %>%
  arrange(count)

glance(mod_1)
```

###Exercise 5

```{r applied model}
test_predictions <- as.tibble(predict(mod_1, newdata = test_set))
test_set1 <- mutate(test_set, PSalePrice = test_predictions$value)
test_set1 <- test_set1[c("GrLivArea", "OverallQual", "Neighborhood", "SalePrice", "PSalePrice")]

price_difference <- rep(NA, 438)
for(i in 1:438){
price_difference[i] <- ((test_set1$SalePrice[i] - test_set1$PSalePrice[i])^2)
}
price_difference <- as.tibble(price_difference)

price_difference %>%
  summarise(sqrt(mean(value))) -> rmse
rmse
```

###Exercise 6

```{r}
mod_1.5 <- lm(SalePrice ~ LotArea + OverallQual + YearRemodAdd, data = train_set)
tidy(mod_1.5)
glance(mod_1.5)

test_predictions_1.5 <- as.tibble(predict(mod_1.5, newdata = test_set))
test_set1.5 <- mutate(test_set, PSalePrice = test_predictions_1.5$value)
test_set1.5 <- test_set1.5[c("LotArea", "OverallQual", "YearRemodAdd", "SalePrice", "PSalePrice")]

price_difference1.5 <- rep(NA, 438)
for(i in 1:438){
price_difference1.5[i] <- ((test_set1.5$SalePrice[i] - test_set1.5$PSalePrice[i])^2)
}
price_difference1.5 <- as.tibble(price_difference1.5)

price_difference1.5 %>%
  summarise(sqrt(mean(value))) -> rmse
rmse
```

###Exercise 7
After running the following model several times, the biggest thing that stood out to me was the variability of the R-squared value. It ranged from 0.50 to 0.94. I added a value of y divided by x to get a rough "feel" for when the sampling provided outliers. It wasn't always true, but for the most part the larger the y/x max value was, the smaller the R-squared value. This makes sense to me because if there are a lot of data points with large y/x values, you would expect the R-squared value to higher than if there was only one or two high y/x outliers.

```{r}
sim1a <- tibble(
  x=rep(1:10, each=3),
  y=x * 1.5 + 6 + rt(length(x), df=2),
  z=y/x
)
max(sim1a$z)
mod_2 <- lm(formula = y ~ x, data = sim1a)
glance(mod_2)
tidy(mod_2)
```



