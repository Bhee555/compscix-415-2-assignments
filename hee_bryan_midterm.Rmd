---
title: "Homework 5/Midterm"
author: "Bryan Hee"
date: "March 3, 2018"
output:
  html_document:
    toc: true
    number_sections: TRUE
---

Github Repository: https://github.com/Bhee555/compscix-415-2-assignments

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
```

#The Tidyverse Packages
##Packages
The following list represents tasks which can be accomplished using the tidyverse library and the corresponding package that is associated with each task:

a. Plotting - ggplot2()

b. Data munging/wrangling - dplyr()

c. Reshaping (spreading and gathering) data - tidyr()

d. Importing/exporting data - readr()

##Functions
The following are 2 functions that we've used from each package listed above:

a. ggplot2() - geom_point(), coord_flip()

b. dplyr() - arrange(), mutate()

c. tidyr() - spread(), gather()

d. readr() - read_delim(), write_rds()


#R Basics
##Code Edit 1
The exclamation mark from the original object name was removed 
```{r}
My_data.name___is.too00ooLong <- c(1,2,3)
```

##Code Edit 2
The "it" was missing a closing parenthesis. The c defining the struct was capitalized.

```{r}
my_string <- c('has', 'an', 'error', 'in', 'it')
```

##Vector Storage
The entire vector is stored as characters rather than numerics (this differed from my expectation that only 3 and 4 would be stored as characters).

```{r}
my_vector <- c(1,2, '3', '4',5)
my_vector
is.numeric(my_vector[1])
```


#Data Import/Export
##Rail Trail Import
Import Rail Trail.txt:
```{r import, message=FALSE}
holder <- 'C:/Users/BryanHee/OneDrive - stok LLC/Intro to Data Science/HW Assignments/Assignment 5/rail_trail.txt'
rail_trail <- read_delim(file = holder, '|')
rm(holder)
glimpse(rail_trail)
```

##Rail Trail Export
Export Rail Trail.rds:
```{r export}
write_rds(rail_trail, path = 'C:/Users/BryanHee/OneDrive - stok LLC/Intro to Data Science/HW Assignments/Assignment 5/rail_trail.rds')
read_rds('C:/Users/BryanHee/OneDrive - stok LLC/Intro to Data Science/HW Assignments/Assignment 5/rail_trail.rds') %>%
glimpse()
```


#Visualization
##Mrs. President Graph Critique
The first critique of the graph provided is that on each of the age bins, the data is provided in percentage of respondents, however the overall data doesn't add up to 100%. I would assume that the remainder percentages of survey respondents replied with answers other than Yes/No, however, I believe it is the job of the data scientist to remove the need for any and all assumptions, such as this.

The second issue i have with the graphic is that there is no indication of the sample sizes for each of the respondent bins.

A third feature that i feel this graphic failed on, are the arbitrary color choices made for the men and women respondent bins. All of the age groups were black, yet genders get two colors? It took me a second to interpret the data because of this.

##Reproduced Plot
Here is the reproduced plot from the diamonds dataset.
```{r}
test <- filter(diamonds, color == "D")

ggplot(diamonds,mapping = aes(cut, carat, fill = color)) +
  geom_boxplot(position = "identity") +
  labs(x = 'CUT OF DIAMOND', y = 'CARAT OF DIAMOND') +
  coord_flip()
#out of all the questions, this took me the longest :), should have known position = "identity"
```


##More Useful Plot
I think the following is a more useful plot because I can now visually see each color's carat size distribution while still being grouped by cut. By getting rid of position = "identity" the geom boxplot automatically separates each color.

```{r}
ggplot(diamonds, mapping = aes(cut, carat, fill = color)) +
  geom_boxplot() +
  labs(x = 'CUT OF DIAMOND', y = 'CARAT OF DIAMOND') +
  coord_flip()
```

#Data Munging and Wrangling
##Table2
No table2 is not currently tidy because the type column includes two variables.
```{r}
spread(table2, key = 'type', value = 'count')
```

##Price per Carat
```{r Price per Carat}
diamonds <- mutate(diamonds, 
       price_per_carat = price / carat)
```


##Pricy and Small Diamonds
The following data represents the number of diamonds, grouped by cut, that cost more than $10,000 and are smaller than 1.5 carats. For the most part, the distribution makes sense to me (see the plot below). This is because as the quality of cut goes up, you would also expect the price of the same size diamond to go up. However it is interesting to me that premium diamonds actually have a smaller proportion than Very Good diamonds do. I would be wary of the fair and good numbers since the overall sample sizes included in the diamonds dataset for these two cuts are much smaller than the other cuts.

```{r}
pricy_small_diamonds <- diamonds %>%
  group_by(cut) %>%
  summarise(pricy_and_small = sum(price > 10000 & carat <1.5),
            total = n()) %>%
  mutate(proportion = pricy_and_small / total)
pricy_small_diamonds
```

```{r}
ggplot(pricy_small_diamonds, mapping = aes(x= cut, y = proportion)) +
  geom_point() +
  labs(title = "Proportion of Small but Expensive Diamonds \nto overall diamonds in dataset")
```


#EDA - Exploratory Data Analysis
##Texas Housing Sales Dataset Time Period
The data extends from January 2000 to July 2015
```{r}
texas_housing <- txhousing %>%
arrange(date)
head(texas_housing, 1)
tail(texas_housing, 1)
```


##Number of Texas Cities in Dataset
There are 46 cities represented in the data set.
```{r}
texas_housing %>%
summarise(n_distinct(city))
```

##Highest Texas Housing Sales
The highest sales, 8945, in the dataset occured in Houston in July 2015. This was the last month of sales data in the dataset.

```{r}
texas_housing %>%
  arrange(desc(sales)) %>%
  head(1)
```


##Texas Housing Sales Efficiency
I would expect the number of sales to go up as the number of listings go up. However, judging by the sales_efficiency metric defined below, as well as the scatterplot, that is not the case. There is no visible trend in the plot between the number of listings and the number of sales.

```{r}
texas_housing %>%
  group_by(date) %>%
  summarise(sales = sum(sales, na.rm = TRUE),
            listings = sum(listings, na.rm = TRUE)) %>%
  mutate(sales_efficiency = sales / listings) -> texas_housing_sales
texas_housing_sales
```

```{r}
  ggplot(texas_housing_sales, mapping = aes(x = listings, y = sales)) +
  geom_point() +
  labs(title = "Texas Housing Sales from Jan 2000 through July 2015", x = "Number of Listings", y = "Number of Sales")
```


##Missing Sales
There are 568 rows of missing data for sales within the dataset. The "testing" tibble below lists the sum of missing sales entries for each city (sorted in descending order by number of missing entries) as well as the proportion of missing to overall sales entries by city.
```{r}
texas_housing %>%
  group_by(city) %>%
  summarise(missing = sum(is.na(sales)),
            total = n()) %>%
  mutate(proportion = missing / total) -> missing_sales
missing_sales %>%
arrange(desc(missing))
summarise(missing_sales, sum(missing))
```


##Median Home Prices in Months w/ Sales > 500
The median house prices are different based on city. As the plot displays below some cities have drastically large ranges, while others have small ranges. Corpus Christi for instance, only has 5 months of data within the filtered dataset. Because of this, it makes more sense that range of data is the smallest for this city. Fort Bend, and Collin County have the largest ranges within their respective median home sale price data. I am most surprised that Collin County has the highest median house sale price among the dataset because i would have though higher density cities such as Houston or Austin would be higher.

The cities that i would want to look into further are Collin County and Fort Worth because they have the highest and lowest median house prices in the filtered data. I'd also want to look into Corpus Christi because there are only 5 months in which there were more than 500 sales.

I would filter out cities / months with less than 500 sales because what we are trying to gauge is the average housing price for the city over a ~15 year period of data. Although median is a fairly robust metric for looking at housing prices, if there are only a few housing sales in any given city / month, there may be specific events that would skew the data. However, with more than 500 sales, it is safe to assume that the sample size is large enough to provide a fair representation of the housing market for that specific city / month.

```{r}
texas_housing2 <- texas_housing
names(texas_housing2)[6]<-"med"
texas_housing2 %>%
  filter(sales > 500) %>%
  group_by(city) -> tex_hou_fil 
summarise(tex_hou_fil, median = median(med), max = max(med), min = min(med), range = (max(med) - min(med)), count = n()) %>%
  arrange(desc(range))
  ggplot(tex_hou_fil, mapping = aes(x = reorder(city, med, median), y = med)) +
  geom_boxplot() +
  coord_flip() +
  labs(title = "Median Home Prices Between 2000 and 2015 \n(months w/ sales > 500)", x = "City", y = "Median Home Price") +
  theme(plot.title = element_text(hjust = 0.5))
```

