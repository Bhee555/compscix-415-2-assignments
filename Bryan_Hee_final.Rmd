---
title: "COMPSCIX 415.2 Homework 9/Final"
author: "Bryan Hee"
date: "April 4, 2018"
output:
  html_document:
    number_sections: no
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
library(rpart)
library(partykit)
library(randomForest)
library(ROCR)
```

#Bootstrapping

###Question 1
Loading the Titanic dataset and forcing all character columns to factors. Also forcing the Survived column to a factor: 

```{r Load Data}
Titanic_R2 <- read.csv(file = "C:/Users/BryanHee/OneDrive - stok LLC/Intro to Data Science/HW Assignments/Assignment 9/train.csv")

Titanic_R2[,sapply(Titanic_R2, is.character)] <- lapply(Titanic_R2[,sapply(Titanic_R2, is.character)], as.factor)

Titanic_R2$Survived <- factor(Titanic_R2$Survived, ordered = FALSE)
Titanic_R2 %>%
  glimpse()
```

###Question 2
Creating a 100 dataset bootstrap:
```{r bootstrap x 100}
titanic_boot <- modelr::bootstrap(data = Titanic_R2, n = 100)
is.tibble(titanic_boot)
titanic_boot
```

Verifying that 100 sample sets were created:
```{r Verify list resample 100}
titanic_boot$strap[[100]] %>%
  as.tibble() %>%
  glimpse()
```


###Question 3
Taking distinct count of rows in a sample bootstrap datasets and double checking via for loop that the code is accurate:
```{r Distinct Bootstrap Entry Counts}
as.tibble(titanic_boot$strap[[20]]) %>% n_distinct()
as.tibble(titanic_boot$strap[[40]]) %>% n_distinct()
as.tibble(titanic_boot$strap[[100]]) %>% n_distinct()

##double checking the above code is correct
dummy <- c(20, 40, 100)
double_check <- rep(NA, 3)

for(i in 1:3) {
  as.tibble(titanic_boot$strap[[dummy[i]]]) -> holder
  length(unique(holder$PassengerId)) -> double_check[i]
  rm(holder)
}
double_check
```

###Questions 4 & 5
Plotting the bootstrapped average age:
```{r Age Central Limit Theorem}
age_mean <- function(dataset) {
  data <- as.tibble(dataset) # convert input data set to a tibble
  mean_age <- mean(data$Age, na.rm = TRUE) # take the mean of Age, remove NAs
  return(mean_age) # return the mean value of Age from data
}


# loop through the 100 bootstrap samples and use the age_mean()
# function
all_means <- rep(NA, 100)

# start the loop
for(i in 1:100) {
  all_means[i] <- age_mean(titanic_boot$strap[[i]])
}

# take a look at some of the means you calculated from your samples
head(all_means)

# convert to a tibble so we can use if for plotting
all_means <- tibble(all_means = all_means)

ggplot(all_means) +
  geom_histogram(mapping = aes(x = all_means), binwidth = 0.1) +
  labs(y = "count", x = "average age", title = "bootstrapped average age")
```

Recreating the histogram with different code:
```{r Alternative Way}
##Alternative way
age_means <- rep(NA, 100)
for(i in 1:100){
  as.tibble(titanic_boot$strap[[i]]) -> holder
  mean(holder$Age, na.rm = TRUE) -> age_means[i]
  rm(holder)
}

age_means <- as.tibble(age_means)

ggplot(data = age_means) +
  geom_histogram(mapping = aes(x = value), binwidth = 0.1) +
  labs(y = "count", x = "average age", title = "bootstrapped average age")
```

###Question 6
The empirical standard error is 3x smaller than the thoeretical standard error.

```{r Standard Error}
SE_theory <- sd(Titanic_R2$Age, na.rm = TRUE)/sqrt(as.numeric(count(all_means)))
SE_theory
SE_empire <- sd(all_means$all_means, na.rm = TRUE)
SE_empire
```

#Random Forest
Creating a test and training set of the Titanic dataset:
###Question 1
```{r Data Split}
set.seed(987)

model_data <- resample_partition(Titanic_R2, c(test = 0.3, train = 0.7))
train_set <- as.tibble(model_data$train)
test_set <- as.tibble(model_data$test)
```

###Question 2
Compared to last week's 3-feature tree, this tree has additional nodes and male are split by age. However, for the most part it looks fairl similar. See below for the resultant tree model:

```{r Decision Tree}
tree_2 <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = train_set)
tree_2
plot(as.party(tree_2))
```

###Question 3
Create a random forest model of the same data:
```{r Random Forest}
rf_mod <- randomForest(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, 
                         data = train_set, 
                         ntrees = 500, 
                         mtry = 4, 
                         na.action = na.roughfix)
rf_mod
```

###Question 4
As expected the random forest represents a better fitting model due to the higher AUC.

```{r Tree vs Forest}
rf_preds <- predict(rf_mod, newdata = test_set, type = 'prob')[,2]
tree_preds <- predict(tree_2, newdata = test_set)[,2]

pred_rf <- prediction(predictions = rf_preds, labels = test_set$Survived)
pred_tree <- prediction(predictions = tree_preds, labels = test_set$Survived)

auc_rf <- performance(prediction.obj = pred_rf, measure = "auc")
auc_tree <- performance(prediction.obj = pred_tree, measure = "auc")

# extract the AUC value
auc_rf@y.values[[1]]
auc_tree@y.values[[1]]
```


###Question 5
Plotting both ROC curves in the same graph:
```{r ROC Plot, warning=FALSE}
# get the FPR and TPR for the random forest model
perf_rf <- performance(pred_rf, measure = 'tpr', x.measure = 'fpr')
perf_rf_tbl <- tibble(perf_rf@x.values[[1]], perf_rf@y.values[[1]])

perf_rf_tbl[3] <- factor("random forest", ordered = FALSE)
# Change the names of the columns of the tibble
names(perf_rf_tbl) <- c('fpr', 'tpr', 'model')

# get the FPR and TPR for the simple tree model
perf_tree <- performance(pred_tree, measure = 'tpr', x.measure = 'fpr')
perf_tree_tbl <- tibble(perf_tree@x.values[[1]], perf_tree@y.values[[1]])

perf_tree_tbl[3] <- factor("tree", ordered = FALSE)
# Change the names of the columns of the tibble
names(perf_tree_tbl) <- c('fpr', 'tpr', 'model')

#Combine results into one tibble
rf_vs_tree <- bind_rows(perf_rf_tbl, perf_tree_tbl)
rf_vs_tree <- transform(rf_vs_tree, model = as.factor(model))

# Plotting function for plotting a nice ROC curve using ggplot
plot_roc <- function(perf_tbl) {
  p <- ggplot(data = perf_tbl, aes(x = fpr, y = tpr, color = model)) +
  geom_line() +
  geom_abline(intercept = 0, slope = 1, lty = 3) +
  labs(x = 'False positive rate', y = 'True positive rate') +
  theme_bw()
  return(p)
}
plot_roc(rf_vs_tree)
```

###Question 6
Based on the ROC curves, the random forest model performs better. This is because the AUC of the random forest performance (red) is greater than the simple tree model (blue/green). 

If we attain a true positive rate of ~0.75, the approximate false positive rate for the random forest model is  0.155, whereas the false positive rate for the simple tree model is  0.34.